{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/accepted_2007_to_2018Q4.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dv/nzlg6jm168311nq7f93wfrd80000gn/T/ipykernel_12074/345045037.py:4: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "from src.data.features import select_features, features, DataType\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = select_features(df)\n",
    "desc = df[\"desc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9358fa55d814b06b0c73329db2715df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 16:19:30 INFO: Downloaded file to /Users/arthurtestard/stanza_resources/resources.json\n",
      "2024-09-23 16:19:30 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-09-23 16:19:31 INFO: File exists: /Users/arthurtestard/stanza_resources/en/default.zip\n",
      "2024-09-23 16:19:33 INFO: Finished downloading models and saved to /Users/arthurtestard/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# May use the package for finance :\n",
    "# https://stanfordnlp.github.io/stanza/client_properties.html#:~:text=FINANCE_PROPS%20%3D%20%7B%0A%20%20%20%20%22depparse.model%22%3A%20%22/path/to/finance%2Dparser.gz%22%2C%0A%20%20%20%20%22ner.model%22%3A%20%22/path/to/finance%2Dner.ser.gz%22%0A%7D%0A%0Awith%20CoreNLPClient()%20as%20client%3A%0A%20%20%20%20bio_ann%20%3D%20client.annotate(bio_text%2C%20properties%3DBIOMEDICAL_PROPS)%0A%20%20%20%20finance_ann%20%3D%20client.annotate(finance_text%2C%20properties%3DFINANCE_PROPS)\n",
    "# from stanza.server import CoreNLPClient\n",
    "# FINANCE_PROPS = {\n",
    "#     \"depparse.model\": \"/path/to/finance-parser.gz\",\n",
    "#     \"ner.model\": \"/path/to/finance-ner.ser.gz\"\n",
    "# }\n",
    "\n",
    "# with CoreNLPClient() as client:\n",
    "#     finance_ann = client.annotate(finance_text, properties=FINANCE_PROPS)\n",
    "\n",
    "stanza.download('en') # download English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nlp = stanza.Pipeline('en', processors=\"tokenize\") # initialize English neural pipeline\n",
    "# doc = nlp(\"Barack Obama was born in Hawaii.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc.tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1\n",
    "# for sentence in doc.sentences:\n",
    "#     print(f\"sentence {k}\")\n",
    "#     print(sentence.tokens)\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 16:19:55 WARNING: Directory /Users/arthurtestard/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    }
   ],
   "source": [
    "stanza.install_corenlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note:\n",
    "It is highly advised to start the server in a context manager (e.g. with CoreNLPClient(...) as client:) to ensure the server is properly shut down when your Python application finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 16:20:02 INFO: Writing properties to tmp file: corenlp_server-d849aae38d79459b.props\n",
      "2024-09-23 16:20:02 INFO: Starting server with command: java -Xmx5G -cp /Users/arthurtestard/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 8000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-d849aae38d79459b.props -preload -outputFormat serialized\n",
      "[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---\n",
      "[main] INFO CoreNLP - Server default properties:\n",
      "\t\t\t(Note: unspecified annotator properties are English defaults)\n",
      "\t\t\tinputFormat = text\n",
      "\t\t\toutputFormat = serialized\n",
      "\t\t\tprettyPrint = false\n",
      "\t\t\tthreads = 5\n",
      "[main] INFO CoreNLP - Threads: 5\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP.properties ... found.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words-distsim.tagger ... done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.2 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.\n",
      "[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580705 unique entries out of 581864 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4867 unique entries out of 4867 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585572 unique entries from 2 files\n",
      "[main] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - numeric classifiers: true; SUTime: true [no docDate]; fine grained: true\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... Time elapsed: 0.5 sec\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 20000 vectors, elapsed Time: 0.518 sec\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [1.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref\n",
      "[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Using mention detector type: dependency\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator kbp\n",
      "[main] INFO edu.stanford.nlp.pipeline.KBPAnnotator - Loading KBP classifier from: edu/stanford/nlp/models/kbp/english/tac-re-lr.ser.gz\n",
      "[main] INFO CoreNLP - Starting server...\n",
      "[main] INFO CoreNLP - StanfordCoreNLPServer listening at /[0:0:0:0:0:0:0:0]:8000\n",
      "[pool-1-thread-3] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:58580] API call w/annotators <unknown>\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP.properties ... found.\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator kbp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.\n"
     ]
    }
   ],
   "source": [
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "with CoreNLPClient(\n",
    "        # annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
    "        # timeout=30000,\n",
    "        # memory='6G'\n",
    "        endpoint=\"http://localhost:8000\"\n",
    "        ) as client:\n",
    "    ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ann.sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris\n",
      "Manning\n",
      "is\n",
      "a\n",
      "nice\n",
      "person\n",
      ".\n",
      "Chris\n",
      "wrote\n",
      "a\n",
      "simple\n",
      "sentence\n",
      ".\n",
      "He\n",
      "also\n",
      "gives\n",
      "oranges\n",
      "to\n",
      "people\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "len(sentence.token)\n",
    "for sentence in ann.sentence:\n",
    "    for tok in sentence.token:\n",
    "        print(tok.value)\n",
    "# sentence.token[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Borrower added on 12/18/11 > I am planning on using the funds to pay off two retail credit cards with 24.99% interest rates, as well as a major bank credit card with a 18.99% rate.  I pay all my bills on time, looking for a lower combined payment and lower monthly payment.<br>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 3\n",
    "desc.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 16:27:25 INFO: Writing properties to tmp file: corenlp_server-ffbfd7f4aedf41ea.props\n",
      "2024-09-23 16:27:25 INFO: Starting server with command: java -Xmx5G -cp /Users/arthurtestard/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 8000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-ffbfd7f4aedf41ea.props -preload -outputFormat serialized\n",
      "[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---\n",
      "[main] INFO CoreNLP - Server default properties:\n",
      "\t\t\t(Note: unspecified annotator properties are English defaults)\n",
      "\t\t\tinputFormat = text\n",
      "\t\t\toutputFormat = serialized\n",
      "\t\t\tprettyPrint = false\n",
      "\t\t\tthreads = 5\n",
      "[main] INFO CoreNLP - Threads: 5\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP.properties ... found.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words-distsim.tagger ... done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.6 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.\n",
      "[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 580705 unique entries out of 581864 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab, 0 TokensRegex patterns.\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 4867 unique entries out of 4867 from edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab, 0 TokensRegex patterns.\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - ner.fine.regexner: Read 585572 unique entries from 2 files\n",
      "[main] INFO edu.stanford.nlp.pipeline.NERCombinerAnnotator - numeric classifiers: true; SUTime: true [no docDate]; fine grained: true\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... Time elapsed: 0.6 sec\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 20000 vectors, elapsed Time: 0.485 sec\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [1.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref\n",
      "[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.CorefMentionAnnotator - Using mention detector type: dependency\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator kbp\n",
      "[main] INFO edu.stanford.nlp.pipeline.KBPAnnotator - Loading KBP classifier from: edu/stanford/nlp/models/kbp/english/tac-re-lr.ser.gz\n",
      "[main] INFO CoreNLP - Starting server...\n",
      "[main] INFO CoreNLP - StanfordCoreNLPServer listening at /[0:0:0:0:0:0:0:0]:8000\n",
      "[pool-1-thread-3] INFO CoreNLP - [/[0:0:0:0:0:0:0:1]:58911] API call w/annotators <unknown>\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Searching for resource: StanfordCoreNLP.properties ... found.\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref\n",
      "[pool-1-thread-3] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator kbp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Borrower added on 12/18/11 > I am planning on using the funds to pay off two retail credit cards with 24.99% interest rates, as well as a major bank credit card with a 18.99% rate.  I pay all my bills on time, looking for a lower combined payment and lower monthly payment.<br>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.\n"
     ]
    }
   ],
   "source": [
    "text = desc.iloc[idx]\n",
    "with CoreNLPClient(\n",
    "        # annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'],\n",
    "        # timeout=30000,\n",
    "        # memory='6G'\n",
    "        endpoint=\"http://localhost:8000\"\n",
    "        ) as client:\n",
    "    ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Borrower\n",
      "added\n",
      "on\n",
      "12/18/11\n",
      ">\n",
      "I\n",
      "am\n",
      "planning\n",
      "on\n",
      "using\n",
      "the\n",
      "funds\n",
      "to\n",
      "pay\n",
      "off\n",
      "two\n",
      "retail\n",
      "credit\n",
      "cards\n",
      "with\n",
      "24.99\n",
      "%\n",
      "interest\n",
      "rates\n",
      ",\n",
      "as\n",
      "well\n",
      "as\n",
      "a\n",
      "major\n",
      "bank\n",
      "credit\n",
      "card\n",
      "with\n",
      "a\n",
      "18.99\n",
      "%\n",
      "rate\n",
      ".\n",
      "I\n",
      "pay\n",
      "all\n",
      "my\n",
      "bills\n",
      "on\n",
      "time\n",
      ",\n",
      "looking\n",
      "for\n",
      "a\n",
      "lower\n",
      "combined\n",
      "payment\n",
      "and\n",
      "lower\n",
      "monthly\n",
      "payment\n",
      ".\n",
      "<br>\n"
     ]
    }
   ],
   "source": [
    "len(sentence.token)\n",
    "for sentence in ann.sentence:\n",
    "    for tok in sentence.token:\n",
    "        print(tok.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Borrower',\n",
       " 'added',\n",
       " 'on',\n",
       " '12/18/11',\n",
       " '>',\n",
       " 'I',\n",
       " 'am',\n",
       " 'planning',\n",
       " 'on',\n",
       " 'using',\n",
       " 'the',\n",
       " 'funds',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'off',\n",
       " 'two',\n",
       " 'retail',\n",
       " 'credit',\n",
       " 'cards',\n",
       " 'with',\n",
       " '24.99',\n",
       " '%',\n",
       " 'interest',\n",
       " 'rates',\n",
       " ',',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'a',\n",
       " 'major',\n",
       " 'bank',\n",
       " 'credit',\n",
       " 'card',\n",
       " 'with',\n",
       " 'a',\n",
       " '18.99',\n",
       " '%',\n",
       " 'rate',\n",
       " '.',\n",
       " 'I',\n",
       " 'pay',\n",
       " 'all',\n",
       " 'my',\n",
       " 'bills',\n",
       " 'on',\n",
       " 'time',\n",
       " ',',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'a',\n",
       " 'lower',\n",
       " 'combined',\n",
       " 'payment',\n",
       " 'and',\n",
       " 'lower',\n",
       " 'monthly',\n",
       " 'payment',\n",
       " '.',\n",
       " '<br>']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [tok.value for sentence in ann.sentence for tok in sentence.token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_core_web_lg package provides 300-dimensional GloVe vectors for 685k terms of English.\n",
    "# https://github.com/explosion/spaCy/blob/2f1e7ed09aae2dcb86a2f36c2df93ad6b4cb3d38/website/docs/usage/linguistic-features.mdx#L1875:~:text=For%20instance%2C%20the%20en_core_web_lg%20package%20provides%20300%2Ddimensional%20GloVe%20vectors%20for%20685k%20terms%20of%20English.\n",
    "\n",
    "glove_nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun GPE\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = glove_nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(company,\n",
       " array([-1.5166  , -3.4286  , -2.4425  ,  2.5937  ,  2.3924  ,  0.267   ,\n",
       "         1.2909  ,  2.9518  , -1.4686  , -0.67879 ,  4.9948  ,  3.3309  ,\n",
       "        -5.664   ,  3.4813  , -3.235   ,  2.4852  ,  3.4837  ,  3.9524  ,\n",
       "        -1.4069  , -0.37771 ,  2.6187  ,  5.2009  , -4.3907  ,  5.2151  ,\n",
       "         1.2805  , -3.4305  , -1.7595  , -0.21102 , -2.4536  , -3.2956  ,\n",
       "         2.3587  ,  0.60871 , -1.5849  ,  4.7089  ,  0.017062,  0.68404 ,\n",
       "         0.72937 ,  0.69773 ,  2.8112  , -1.7209  ,  1.1287  ,  2.435   ,\n",
       "         1.736   ,  1.7813  ,  0.35448 ,  2.9205  ,  3.3382  , -1.6494  ,\n",
       "         1.8682  ,  1.161   , -0.77744 ,  2.1459  ,  0.6653  , -7.1686  ,\n",
       "         1.1688  , -2.0678  , -2.8886  ,  1.0852  , -5.884   , -0.71366 ,\n",
       "         3.3266  , -0.68555 ,  0.92449 , -0.63958 ,  3.5797  , -0.1035  ,\n",
       "        -1.1236  , -2.4793  ,  2.5927  ,  3.604   , -2.345   ,  2.4805  ,\n",
       "        -0.19703 , -1.2971  ,  2.4587  ,  0.58073 , -1.9183  ,  5.9099  ,\n",
       "         0.07689 ,  2.311   , -0.24159 ,  3.819   ,  2.8975  ,  2.4171  ,\n",
       "        -0.85476 ,  1.8193  , -0.57626 , -0.49632 , -1.9566  , -1.278   ,\n",
       "        -0.92861 ,  0.86816 , -2.3663  , -2.475   ,  2.6372  , -1.3985  ,\n",
       "        -0.37513 , -0.49427 ,  3.1876  ,  2.0724  ,  6.4552  , -4.6169  ,\n",
       "         0.87978 ,  1.9511  ,  1.2796  ,  4.6803  , -4.7857  , -2.2791  ,\n",
       "        -1.3933  , -9.5098  ,  4.7161  ,  0.86369 , -0.32059 , -2.812   ,\n",
       "         3.0464  ,  4.3754  , -3.421   , -1.7862  ,  1.8138  , -6.373   ,\n",
       "         1.8297  , -2.5099  ,  0.85415 ,  3.129   ,  1.2262  , -2.7166  ,\n",
       "        -1.8661  , -2.1386  ,  5.5327  , -2.8714  , -0.74454 , -0.89037 ,\n",
       "         4.5867  ,  4.6342  , -3.598   , -0.14538 ,  1.3946  , -1.4402  ,\n",
       "         3.3551  ,  1.7117  , -4.2924  , -2.4303  ,  1.2628  ,  4.3818  ,\n",
       "         0.85033 , -0.98227 , -2.7176  ,  1.3922  , -2.2519  ,  5.7688  ,\n",
       "        -2.1452  , -1.5892  , -0.42541 ,  2.3746  , -4.0373  ,  1.3206  ,\n",
       "         3.2763  ,  2.5817  , -1.9234  , -0.32937 , -1.3761  , -4.581   ,\n",
       "         0.48942 ,  2.4339  , -1.9613  , -1.9741  , -1.4389  ,  2.9693  ,\n",
       "         1.1527  , -2.985   , -3.3727  , -4.5778  , -2.8753  ,  4.0621  ,\n",
       "         1.8916  ,  6.2157  ,  1.358   , -1.3575  , -3.0241  ,  0.46293 ,\n",
       "        -3.142   ,  2.9516  ,  2.9491  , -2.2807  , -1.1218  ,  0.60336 ,\n",
       "        -0.84722 , -2.6147  ,  4.789   ,  1.7633  ,  0.80365 , -2.7098  ,\n",
       "        -1.2847  , -4.273   ,  6.4017  ,  3.0676  , -1.97    ,  3.4292  ,\n",
       "         1.3282  ,  2.1914  ,  1.7475  , -0.27758 ,  2.7634  ,  1.649   ,\n",
       "         2.6068  ,  4.1264  , -1.8203  ,  2.0835  ,  0.79561 , -2.5045  ,\n",
       "        -0.73937 , -1.1426  , -0.6804  ,  4.2886  ,  3.3416  , -0.36997 ,\n",
       "         4.2049  , -8.0268  , -1.077   ,  0.48453 , -3.5177  , -1.2762  ,\n",
       "        -3.602   , -4.7908  , -3.2093  , -4.7346  , -1.2684  ,  3.2961  ,\n",
       "        -0.17655 , -3.1755  ,  6.8025  , -1.3887  , -2.6579  ,  5.3194  ,\n",
       "         6.8584  , -0.89899 , -3.9659  , -0.75605 , -0.24533 ,  3.4498  ,\n",
       "        -1.5116  , -0.43963 ,  2.1807  ,  0.44292 , -1.613   ,  1.4647  ,\n",
       "         1.541   ,  1.336   , -0.30598 ,  2.5524  , -0.16534 , -3.3844  ,\n",
       "        -6.754   , -3.1545  , -2.0155  , -1.9435  ,  0.90694 , -5.6109  ,\n",
       "        -1.1579  ,  1.7793  , -2.4413  ,  7.4121  ,  2.0829  ,  0.73811 ,\n",
       "         1.6112  ,  1.1925  ,  0.18722 ,  4.1867  ,  3.725   , -2.0991  ,\n",
       "        -1.7269  ,  0.78542 , -0.93918 , -1.3053  ,  3.0589  , -6.6806  ,\n",
       "        -0.26277 , -2.3553  , -1.7703  ,  0.87562 ,  7.5939  ,  3.3228  ,\n",
       "         4.3611  , -1.2967  , 10.795   ,  3.5001  ,  0.31363 ,  2.798   ,\n",
       "        -4.6883  , -0.040859, -1.434   , -1.6175  , -2.6483  ,  3.0967  ,\n",
       "        -7.9933  , -3.476   ,  2.229   , -1.911   , -0.85713 ,  3.1659  ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 20\n",
    "doc[id], doc[id].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "emb = np.array([ glove_nlp.vocab.get_vector(tok) for tok in tokens ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59, 300)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
